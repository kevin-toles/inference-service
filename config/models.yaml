# =============================================================================
# AI Models Registry
# =============================================================================
# This file defines all available models and their metadata.
# Actual model files are stored in ../models/ and are gitignored.
# =============================================================================

version: "1.0.0"

models:
  phi-4:
    name: "Microsoft Phi-4"
    description: "General reasoning, summarization, coding"
    file: "phi-4/phi-4-Q4_K_S.gguf"
    size_gb: 8.4
    context_length: 4096  # Reduced - full 16K context needs more RAM
    quantization: "Q4_K_S"
    source:
      repo: "microsoft/phi-4-gguf"
      filename: "phi-4-Q4_K_S.gguf"
    roles:
      - primary
      - thinker
      - coder
    # GPU layers: -1=all GPU (Metal), 0=CPU only, N=hybrid (N layers on GPU)
    # phi-4 has 40 layers. Optimal for 16GB Mac: 35 GPU + 5 CPU = best speed
    gpu_layers: 35  # Optimal hybrid: 35/40 layers on GPU (~26s vs 57s CPU-only)

  deepseek-r1-7b:
    name: "DeepSeek R1 Distill 7B"
    description: "Chain-of-thought reasoning, complex analysis"
    file: "deepseek-r1-7b/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    size_gb: 4.7
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF"
      filename: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    roles:
      - thinker
    gpu_layers: -1  # All layers on GPU (Metal)

  qwen2.5-7b:
    name: "Qwen 2.5 7B Instruct"
    description: "Code generation, technical tasks"
    file: "qwen2.5-7b/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
    size_gb: 4.5
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "Qwen/Qwen2.5-7B-Instruct-GGUF"
      filename: "qwen2.5-7b-instruct-q4_k_m.gguf"
    roles:
      - coder
      - primary
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  llama-3.2-3b:
    name: "Llama 3.2 3B Instruct"
    description: "Fast responses, simple queries"
    file: "llama-3.2-3b/llama-3.2-3b-instruct-q4_k_m.gguf"
    size_gb: 2.0
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
      filename: "llama-3.2-3b-instruct-q4_k_m.gguf"
    roles:
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  phi-3-medium-128k:
    name: "Phi-3 Medium 128K Instruct"
    description: "Long context processing (128K), document analysis"
    file: "phi-3-medium-128k/Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    size_gb: 8.6
    context_length: 8192  # Reduced - full 128K needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "bartowski/Phi-3-medium-128k-instruct-GGUF"
      filename: "Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    roles:
      - longctx
      - thinker
    gpu_layers: 0  # CPU-only - 8.6GB model needs CPU mode on 16GB Mac
    notes: "Full 128K context requires 32GB+ RAM"

  gpt-oss-20b:
    name: "GPT-OSS 20B"
    description: "High-capacity model for complex reasoning (server only)"
    file: "gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf"
    size_gb: 11.6
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/gpt-oss-20b-GGUF"
      filename: "gpt-oss-20b-Q4_K_M.gguf"
    roles:
      - primary
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

  granite-8b-code-128k:
    name: "IBM Granite 8B Code Instruct 128K"
    description: "Code-focused model with 128K context for full-file analysis"
    file: "granite-8b-code-128k/granite-8b-code-instruct-128k.Q4_K_M.gguf"
    size_gb: 4.5
    context_length: 8192  # Reduced - full 128K needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-8b-code-instruct-128k-GGUF"
      filename: "granite-8b-code-instruct-128k.Q4_K_M.gguf"
    roles:
      - coder
      - longctx
    gpu_layers: -1  # Fits in GPU at reduced context
    notes: "Full 128K context requires 32GB+ RAM"

  granite-20b-code:
    name: "IBM Granite 20B Code Instruct"
    description: "High-capacity code model (server only)"
    file: "granite-20b-code/granite-20b-code-instruct.Q4_K_M.gguf"
    size_gb: 12.8
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-20b-code-instruct-GGUF"
      filename: "granite-20b-code-instruct.Q4_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

  # =========================================================================
  # Qwen3 Models (New - December 2025)
  # =========================================================================
  qwen3-8b:
    name: "Qwen3 8B"
    description: "Alibaba Qwen3 8B - balanced reasoning and instruction following"
    file: "qwen3-8b/Qwen3-8B-Q4_K_M.gguf"
    size_gb: 4.9
    context_length: 16384  # Increased from 4096 - needed for Kitchen Brigade prompts with cross-reference evidence
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/Qwen3-8B-GGUF"
      filename: "Qwen3-8B-Q4_K_M.gguf"
    roles:
      - primary
      - thinker
      - fast
    gpu_layers: 28  # Hybrid: 28 layers on GPU for 16GB Mac

  qwen3-coder-30b:
    name: "Qwen3 Coder 30B A3B Instruct"
    description: "Alibaba Qwen3 Coder 30B - advanced code generation and analysis"
    file: "qwen3-coder-30b-a3b/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    size_gb: 14.2
    context_length: 4096  # Reduced - full 32K requires 48GB+ RAM
    quantization: "Q3_K_M"
    source:
      repo: "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
      filename: "Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: 0  # CPU only - 14GB model needs CPU mode on 16GB Mac
    notes: "Requires >24GB RAM/VRAM for GPU - CPU mode on consumer hardware"

  # =========================================================================
  # CodeLlama Models (January 2026)
  # =========================================================================
  codellama-7b-instruct:
    name: "CodeLlama 7B Instruct"
    description: "Meta CodeLlama 7B - code generation with instruction following"
    file: "codellama-7b-instruct/codellama-7b-instruct-q4_k_m.gguf"
    size_gb: 4.0
    context_length: 16384  # Increased from 4096 - needed for Kitchen Brigade prompts with cross-reference evidence
    quantization: "Q4_K_M"
    source:
      repo: "TheBloke/CodeLlama-7B-Instruct-GGUF"
      filename: "codellama-7b-instruct.Q4_K_M.gguf"
    roles:
      - coder
      - primary
    gpu_layers: -1  # All layers on GPU (Metal)

  codellama-13b:
    name: "CodeLlama 13B"
    description: "Meta CodeLlama 13B - larger capacity code generation"
    file: "codellama-13b/codellama-13b-q4_k_m.gguf"
    size_gb: 7.5
    context_length: 4096
    quantization: "Q4_K_M"
    source:
      repo: "TheBloke/CodeLlama-13B-GGUF"
      filename: "codellama-13b.Q4_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: 30  # Hybrid: 30 layers on GPU for 16GB Mac
    notes: "Tight fit on 16GB - close other apps before use"

  qwen2.5-coder-7b:
    name: "Qwen 2.5 Coder 7B Instruct"
    description: "Alibaba Qwen 2.5 Coder - specialized for code generation and reasoning"
    file: "qwen2.5-coder-7b/qwen2.5-coder-7b-instruct-q4_k_m.gguf"
    size_gb: 4.7
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF"
      filename: "qwen2.5-coder-7b-instruct-q4_k_m.gguf"
    roles:
      - coder
      - primary
    gpu_layers: -1  # All layers on GPU (Metal)

  starcoder2-7b:
    name: "StarCoder2 7B"
    description: "BigCode StarCoder2 - strong mid-size coder trained on The Stack v2"
    file: "starcoder2-7b/starcoder2-7b-q4_k_m.gguf"
    size_gb: 4.1
    context_length: 16384
    quantization: "Q4_K_M"
    source:
      repo: "second-state/StarCoder2-7B-GGUF"
      filename: "starcoder2-7b-Q4_K_M.gguf"
    roles:
      - coder
      - primary
    gpu_layers: -1  # All layers on GPU (Metal)

  codegemma-7b:
    name: "CodeGemma 7B IT"
    description: "Google CodeGemma - code-tuned Gemma variant with instruction tuning"
    file: "codegemma-7b/codegemma-7b-it-q4_k_m.gguf"
    size_gb: 5.0
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "bartowski/codegemma-7b-it-GGUF"
      filename: "codegemma-7b-it-Q4_K_M.gguf"
    roles:
      - coder
      - primary
    gpu_layers: -1  # All layers on GPU (Metal)

  deepseek-coder-v2-lite:
    name: "DeepSeek Coder V2 Lite Instruct"
    description: "DeepSeek MoE code model - 32K context for 16GB Mac"
    file: "deepseek-coder-v2-lite/deepseek-coder-v2-lite-instruct-q4_k_m.gguf"
    size_gb: 9.0
    context_length: 32768  # Reduced from 128K - keeps RAM ~12GB on 16GB Mac
    quantization: "Q4_K_M"
    source:
      repo: "bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF"
      filename: "DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: -1  # All layers on GPU (Metal) - MoE only activates ~2.4B params
    notes: "MoE architecture (16B total, 2.4B active) - full 128K needs 32GB+ RAM"

# =============================================================================
# Role Definitions
# =============================================================================
roles:
  primary:
    description: "Default/general purpose model"
    task_types:
      - general
      - summarize
      - explain
      - chat

  fast:
    description: "Quick responses, simple tasks"
    task_types:
      - simple
      - quick
      - classify
      - extract

  coder:
    description: "Code generation and review"
    task_types:
      - code
      - debug
      - review
      - refactor
      - test

  thinker:
    description: "Complex reasoning and analysis"
    task_types:
      - analyze
      - reason
      - compare
      - debate
      - plan

  longctx:
    description: "Long document processing"
    task_types:
      - document
      - summarize_long
      - rag
