# =============================================================================
# AI Models Registry
# =============================================================================
# This file defines all available models and their metadata.
# Actual model files are stored in ../models/ and are gitignored.
# =============================================================================

version: "1.0.0"

models:
  phi-4:
    name: "Microsoft Phi-4"
    description: "General reasoning, summarization, coding"
    file: "phi-4/phi-4-Q4_K_S.gguf"
    size_gb: 8.4
    context_length: 4096  # Reduced - full 16K context needs more RAM
    quantization: "Q4_K_S"
    source:
      repo: "microsoft/phi-4-gguf"
      filename: "phi-4-Q4_K_S.gguf"
    roles:
      - primary
      - thinker
      - coder
    # GPU layers: -1=all GPU (Metal), 0=CPU only, N=hybrid (N layers on GPU)
    # phi-4 has 40 layers. Optimal for 16GB Mac: 35 GPU + 5 CPU = best speed
    gpu_layers: 35  # Optimal hybrid: 35/40 layers on GPU (~26s vs 57s CPU-only)

  deepseek-r1-7b:
    name: "DeepSeek R1 Distill 7B"
    description: "Chain-of-thought reasoning, complex analysis"
    file: "deepseek-r1-7b/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    size_gb: 4.7
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF"
      filename: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    roles:
      - thinker
    gpu_layers: -1  # All layers on GPU (Metal)

  qwen2.5-7b:
    name: "Qwen 2.5 7B Instruct"
    description: "Code generation, technical tasks"
    file: "qwen2.5-7b/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
    size_gb: 4.5
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "Qwen/Qwen2.5-7B-Instruct-GGUF"
      filename: "qwen2.5-7b-instruct-q4_k_m.gguf"
    roles:
      - coder
      - primary
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  llama-3.2-3b:
    name: "Llama 3.2 3B Instruct"
    description: "Fast responses, simple queries"
    file: "llama-3.2-3b/llama-3.2-3b-instruct-q4_k_m.gguf"
    size_gb: 2.0
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
      filename: "llama-3.2-3b-instruct-q4_k_m.gguf"
    roles:
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  phi-3-medium-128k:
    name: "Phi-3 Medium 128K Instruct"
    description: "Long context processing (128K), document analysis"
    file: "phi-3-medium-128k/Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    size_gb: 8.6
    context_length: 8192  # Reduced - full 128K needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "bartowski/Phi-3-medium-128k-instruct-GGUF"
      filename: "Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    roles:
      - longctx
      - thinker
    gpu_layers: 0  # CPU-only - 8.6GB model needs CPU mode on 16GB Mac
    notes: "Full 128K context requires 32GB+ RAM"

  gpt-oss-20b:
    name: "GPT-OSS 20B"
    description: "High-capacity model for complex reasoning (server only)"
    file: "gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf"
    size_gb: 11.6
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/gpt-oss-20b-GGUF"
      filename: "gpt-oss-20b-Q4_K_M.gguf"
    roles:
      - primary
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

  granite-8b-code-128k:
    name: "IBM Granite 8B Code Instruct 128K"
    description: "Code-focused model with 128K context for full-file analysis"
    file: "granite-8b-code-128k/granite-8b-code-instruct-128k.Q4_K_M.gguf"
    size_gb: 4.5
    context_length: 8192  # Reduced - full 128K needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-8b-code-instruct-128k-GGUF"
      filename: "granite-8b-code-instruct-128k.Q4_K_M.gguf"
    roles:
      - coder
      - longctx
    gpu_layers: -1  # Fits in GPU at reduced context
    notes: "Full 128K context requires 32GB+ RAM"

  granite-20b-code:
    name: "IBM Granite 20B Code Instruct"
    description: "High-capacity code model (server only)"
    file: "granite-20b-code/granite-20b-code-instruct.Q4_K_M.gguf"
    size_gb: 12.8
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-20b-code-instruct-GGUF"
      filename: "granite-20b-code-instruct.Q4_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

# =============================================================================
# Role Definitions
# =============================================================================
roles:
  primary:
    description: "Default/general purpose model"
    task_types:
      - general
      - summarize
      - explain
      - chat

  fast:
    description: "Quick responses, simple tasks"
    task_types:
      - simple
      - quick
      - classify
      - extract

  coder:
    description: "Code generation and review"
    task_types:
      - code
      - debug
      - review
      - refactor
      - test

  thinker:
    description: "Complex reasoning and analysis"
    task_types:
      - analyze
      - reason
      - compare
      - debate
      - plan

  longctx:
    description: "Long document processing"
    task_types:
      - document
      - summarize_long
      - rag
