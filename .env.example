# =============================================================================
# inference-service Environment Variables
# =============================================================================
# Copy to .env and customize for your environment
# Reference: docs/ARCHITECTURE.md â†’ Configuration Reference

# =============================================================================
# Core Settings
# =============================================================================
INFERENCE_PORT=8085
INFERENCE_HOST=0.0.0.0
INFERENCE_LOG_LEVEL=INFO

# =============================================================================
# Model Storage
# =============================================================================
# Path to directory containing GGUF model files
INFERENCE_MODELS_DIR=/Users/kevintoles/POC/ai-models/models

# =============================================================================
# Model Configuration
# =============================================================================
# Use a preset configuration (S1-S8, D1-D15, T1-T13, Q1-Q7, P1-P6)
# See docs/ARCHITECTURE.md for preset definitions
INFERENCE_CONFIG=D3

# Or specify explicit model list (overrides CONFIG)
# INFERENCE_PRELOAD_MODELS=phi-4,qwen2.5-7b

# =============================================================================
# Orchestration Settings
# =============================================================================
# Mode: single | critique | debate | ensemble | pipeline
INFERENCE_ORCHESTRATION_MODE=single

# Critique mode settings
INFERENCE_CRITIQUE_MAX_ROUNDS=2
INFERENCE_CRITIQUE_THRESHOLD=0.8

# Debate mode settings
INFERENCE_DEBATE_MAX_ROUNDS=3
INFERENCE_DEBATE_RECONCILE_MODEL=primary

# Ensemble mode settings
INFERENCE_ENSEMBLE_MIN_AGREEMENT=0.7
INFERENCE_ENSEMBLE_SYNTHESIS_MODEL=primary

# Pipeline mode settings
INFERENCE_PIPELINE_STAGES=draft,refine,validate

# =============================================================================
# Request Handling
# =============================================================================
INFERENCE_MAX_CONCURRENT_REQUESTS=10
INFERENCE_REQUEST_TIMEOUT=120
# Mac: 1 concurrent per model, Server: 4+
INFERENCE_MAX_CONCURRENT_PER_MODEL=1

# =============================================================================
# Queue Behavior
# =============================================================================
# Strategy: fifo | priority
INFERENCE_QUEUE_STRATEGY=fifo
INFERENCE_REJECT_WHEN_FULL=true

# =============================================================================
# Priority Queue (when QUEUE_STRATEGY=priority)
# =============================================================================
INFERENCE_PRIORITY_ENABLED=false
# Number of priority levels (1=low, 2=normal, 3=high)
INFERENCE_PRIORITY_LEVELS=3
INFERENCE_DEFAULT_PRIORITY=2

# =============================================================================
# Auto-Routing (when multiple models loaded)
# =============================================================================
INFERENCE_AUTO_ROUTE_ENABLED=false
# Strategy: least_busy | round_robin | capability_match
INFERENCE_AUTO_ROUTE_STRATEGY=least_busy
INFERENCE_AUTO_ROUTE_FALLBACK=true

# =============================================================================
# Model Role Mapping
# =============================================================================
# JSON mapping of model ID to roles
# Roles: fast, coder, thinker, primary, longctx
INFERENCE_MODEL_ROLES='{"llama-3.2-3b":["fast"],"qwen2.5-7b":["coder","primary"],"deepseek-r1-7b":["thinker"],"granite-8b-code-128k":["coder","longctx"],"phi-4":["primary","thinker","coder"],"phi-3-medium-128k":["longctx","thinker"],"gpt-oss-20b":["primary","thinker"],"granite-20b-code":["coder","thinker"]}'

# =============================================================================
# Hardware Settings
# =============================================================================
# GPU layers: -1 = all layers on GPU (Metal/CUDA)
INFERENCE_GPU_LAYERS=-1
# Backend: llamacpp | vllm
INFERENCE_BACKEND=llamacpp

# =============================================================================
# Context Management
# =============================================================================
# Maximum tokens to reserve for generation output
INFERENCE_RESERVE_OUTPUT_TOKENS=2048
# Compression strategy: truncate | summarize | key_points
INFERENCE_COMPRESSION_STRATEGY=truncate

# =============================================================================
# Caching
# =============================================================================
INFERENCE_CACHE_ENABLED=true
INFERENCE_PROMPT_CACHE_SIZE=100
INFERENCE_HANDOFF_CACHE_TTL=3600
INFERENCE_COMPRESSION_CACHE_SIZE=50
