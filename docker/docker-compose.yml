# ==============================================================================
# Inference Service - Docker Compose
# ==============================================================================
# Role: LINE COOK - LLM inference with GPU acceleration
# Port: 8085
# Reference: ai-platform-data/docs/NETWORK_ARCHITECTURE.md
# ==============================================================================
#
# ⚠️ PERFORMANCE NOTE:
# Docker on macOS runs in a Linux VM which CANNOT access Apple's Metal GPU.
# Use the Platform Control Panel to toggle between Docker and Native mode.
#
# Performance Comparison:
#   Native (Metal): 30-100 tokens/second
#   Docker (CPU):   0.04 tokens/second (~1000x slower)
#   Docker (CUDA):  30-100 tokens/second (Linux with NVIDIA GPU)
#
# ==============================================================================
#
# Usage:
#   docker-compose up -d                    # Start CPU version
#   docker-compose --profile cuda up -d     # Start with NVIDIA GPU
#   docker-compose logs -f                  # Follow logs
#   docker-compose down                     # Stop service
#
# ==============================================================================

services:
  # ============================================================================
  # Inference Service - CPU Version (Default)
  # ============================================================================
  inference-service:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: inference-service
    ports:
      - "8085:8085"
    environment:
      # Core Service Configuration
      INFERENCE_PORT: "8085"
      INFERENCE_LOG_LEVEL: "INFO"
      INFERENCE_ENVIRONMENT: "development"
      
      # Model/Inference Configuration
      INFERENCE_GPU_LAYERS: "0"  # CPU only (use -1 for CUDA)
      INFERENCE_BACKEND: "llamacpp"
      INFERENCE_ORCHESTRATION_MODE: "single"
      
      # Auto-load preset on startup
      INFERENCE_DEFAULT_PRESET: "${INFERENCE_DEFAULT_PRESET:-D4}"
    volumes:
      - ${MODELS_PATH:-../models}:/app/models:ro
      - ../config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    restart: "no"  # CHANGED: Prevent Docker Desktop auto-start - use native mode
    networks:
      - ai-platform-network
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G

  # ============================================================================
  # Inference Service - CUDA GPU Version
  # Profile: cuda (use --profile cuda to enable)
  # ============================================================================
  inference-service-cuda:
    profiles:
      - cuda
    build:
      context: ..
      dockerfile: docker/Dockerfile.cuda
    container_name: inference-service
    ports:
      - "8085:8085"
    environment:
      INFERENCE_PORT: "8085"
      INFERENCE_LOG_LEVEL: "INFO"
      INFERENCE_ENVIRONMENT: "development"
      INFERENCE_GPU_LAYERS: "-1"  # All layers on GPU
      INFERENCE_BACKEND: "llamacpp"
      INFERENCE_ORCHESTRATION_MODE: "single"
      INFERENCE_DEFAULT_PRESET: "${INFERENCE_DEFAULT_PRESET:-D4}"
    volumes:
      - ${MODELS_PATH:-../models}:/app/models:ro
      - ../config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    restart: "no"  # CHANGED: Prevent Docker Desktop auto-start - use native mode
    networks:
      - ai-platform-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# ==============================================================================
# Networks - Tiered network architecture
# ==============================================================================
networks:
  ai-platform-network:
    name: ai-platform-network
    external: true
