"""inference-service: Self-hosted LLM inference worker.

This package provides local LLM inference using llama-cpp-python (Mac/Metal)
with future support for vLLM (server/CUDA).
"""

__version__ = "0.1.0"
__all__ = ["__version__"]
